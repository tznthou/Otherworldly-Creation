use crate::services::ollama::{get_ollama_service, OllamaOptions, UpdateConfig};
use serde::{Deserialize, Serialize};
use tauri::command;

#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct ServiceStatus {
    pub service: ServiceInfo,
    pub models: ModelsInfo,
    pub last_checked: String,
}

#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct ServiceInfo {
    pub available: bool,
    pub version: Option<String>,
    pub error: Option<String>,
}

#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct ModelsInfo {
    pub count: usize,
    pub list: Vec<String>,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct GenerateParams {
    pub temperature: Option<f32>,
    pub top_p: Option<f32>,
    pub max_tokens: Option<u32>,
    pub presence_penalty: Option<f32>,
    pub frequency_penalty: Option<f32>,
    pub max_context_tokens: Option<u32>,
}

/// æª¢æŸ¥ Ollama æœå‹™æ˜¯å¦å¯ç”¨
#[command]
pub async fn check_ollama_service() -> Result<bool, String> {
    log::info!("=== Tauri Command: æª¢æŸ¥ Ollama æœå‹™ ===");
    
    let ollama_service = get_ollama_service();
    let service = ollama_service.lock().await;
    let result = service.check_service_availability().await;
    
    log::info!("Tauri: Ollama æœå‹™æª¢æŸ¥çµæœ: {:?}", result);
    
    Ok(result.available)
}

/// ç²å–è©³ç´°çš„æœå‹™ç‹€æ…‹
#[command]
pub async fn get_service_status() -> Result<ServiceStatus, String> {
    log::info!("=== é–‹å§‹ç²å–æœå‹™ç‹€æ…‹ ===");
    
    let ollama_service = get_ollama_service();
    let service = ollama_service.lock().await;
    let service_check = service.check_service_availability().await;
    let models_result = service.list_models().await;
    
    let service_info = ServiceInfo {
        available: service_check.available,
        version: service_check.version,
        error: service_check.error,
    };
    
    let models_info = ModelsInfo {
        count: if models_result.success { models_result.models.len() } else { 0 },
        list: if models_result.success {
            models_result.models.iter().map(|m| m.name.clone()).collect()
        } else {
            vec![]
        },
    };
    
    let status = ServiceStatus {
        service: service_info,
        models: models_info,
        last_checked: chrono::Utc::now().to_rfc3339(),
    };
    
    log::info!("æœå‹™ç‹€æ…‹: {:?}", status);
    Ok(status)
}

/// ç²å–æ¨¡å‹åˆ—è¡¨
#[command]
pub async fn list_models() -> Result<Vec<String>, String> {
    log::info!("=== é–‹å§‹ç²å–æ¨¡å‹åˆ—è¡¨ ===");
    
    let ollama_service = get_ollama_service();
    let service = ollama_service.lock().await;
    let result = service.list_models().await;
    log::info!("æ¨¡å‹åˆ—è¡¨çµæœ: {:?}", result);
    
    if result.success {
        let model_names: Vec<String> = result.models.iter().map(|model| model.name.clone()).collect();
        log::info!("å¯ç”¨æ¨¡å‹: {:?}", model_names);
        log::info!("æ¨¡å‹æ•¸é‡: {}", model_names.len());
        Ok(model_names)
    } else {
        log::error!("ç²å–æ¨¡å‹åˆ—è¡¨å¤±æ•—: {:?}", result.error);
        Ok(vec![])
    }
}

/// ç²å–è©³ç´°çš„æ¨¡å‹è³‡è¨Š
#[command]
pub async fn get_models_info() -> Result<crate::services::ollama::ModelsResult, String> {
    log::info!("=== é–‹å§‹ç²å–è©³ç´°æ¨¡å‹è³‡è¨Š ===");
    
    let ollama_service = get_ollama_service();
    let service = ollama_service.lock().await;
    let result = service.list_models().await;
    log::info!("è©³ç´°æ¨¡å‹è³‡è¨Š: {:?}", result);
    Ok(result)
}

/// æª¢æŸ¥ç‰¹å®šæ¨¡å‹æ˜¯å¦å¯ç”¨
#[command]
pub async fn check_model_availability(model_name: String) -> Result<crate::services::ollama::ModelAvailability, String> {
    let ollama_service = get_ollama_service();
    let service = ollama_service.lock().await;
    let result = service.check_model_availability(&model_name).await;
    Ok(result)
}

/// ç”Ÿæˆæ–‡æœ¬
#[command]
pub async fn generate_text(
    prompt: String,
    model: String,
    params: GenerateParams,
) -> Result<String, String> {
    log::info!("=== é–‹å§‹ç”Ÿæˆæ–‡æœ¬ï¼Œæ¨¡å‹: {} ===", model);
    
    let options = OllamaOptions {
        temperature: params.temperature,
        top_p: params.top_p,
        max_tokens: params.max_tokens,
        presence_penalty: params.presence_penalty,
        frequency_penalty: params.frequency_penalty,
    };
    
    let ollama_service = get_ollama_service();
    let service = ollama_service.lock().await;
    let result = service.generate_text(&model, &prompt, Some(options)).await;
    
    if result.success {
        Ok(result.response.unwrap_or_default())
    } else {
        Err(result.error.unwrap_or("ç”Ÿæˆæ–‡æœ¬å¤±æ•—".to_string()))
    }
}

/// ä½¿ç”¨åˆ†é›¢ä¸Šä¸‹æ–‡ç”Ÿæˆæ–‡æœ¬ï¼ˆç°¡åŒ–ç‰ˆï¼‰
#[command]
pub async fn generate_with_separated_context(
    project_id: String,
    chapter_id: String,
    position: usize,
    model: String,
    params: GenerateParams,
) -> Result<String, String> {
    log::info!("=== é–‹å§‹ä½¿ç”¨åˆ†é›¢ä¸Šä¸‹æ–‡ç”Ÿæˆæ–‡æœ¬ï¼ˆç°¡åŒ–ç‰ˆï¼‰===");
    log::info!("å°ˆæ¡ˆ: {}, ç« ç¯€: {}, ä½ç½®: {}, æ¨¡å‹: {}", project_id, chapter_id, position, model);
    
    // 1. æ§‹å»ºåˆ†é›¢çš„ä¸Šä¸‹æ–‡
    let (system_prompt, user_context) = crate::commands::context::build_separated_context(
        project_id.clone(), chapter_id.clone(), position
    )
        .await
        .map_err(|e| format!("æ§‹å»ºåˆ†é›¢ä¸Šä¸‹æ–‡å¤±æ•—: {}", e))?;
    
    log::info!("ç³»çµ±æç¤ºé•·åº¦: {} å­—ç¬¦", system_prompt.len());
    log::info!("ç”¨æˆ¶ä¸Šä¸‹æ–‡é•·åº¦: {} å­—ç¬¦", user_context.len());
    
    // 2. å¦‚æœæœ‰ Chat API æ”¯æ´ï¼Œä½¿ç”¨ Chat API
    // ç›®å‰å…ˆå›é€€åˆ°å‚³çµ±æ–¹å¼ï¼Œä½†çµæ§‹å·²æº–å‚™å¥½ Chat API å‡ç´š
    let combined_prompt = format!("{}\n\n{}", system_prompt, user_context);
    
    // 3. ä½¿ç”¨ä¸Šä¸‹æ–‡ç”Ÿæˆæ–‡æœ¬
    let options = crate::services::ollama::OllamaOptions {
        temperature: params.temperature,
        top_p: params.top_p,
        max_tokens: params.max_tokens,
        presence_penalty: params.presence_penalty,
        frequency_penalty: params.frequency_penalty,
    };
    
    let ollama_service = get_ollama_service();
    let service = ollama_service.lock().await;
    let result = service.generate_text(&model, &combined_prompt, Some(options)).await;
    
    if result.success {
        let generated_text = result.response.unwrap_or_default();
        log::info!("ç”Ÿæˆæ–‡æœ¬æˆåŠŸï¼Œé•·åº¦: {} å­—ç¬¦", generated_text.len());
        Ok(generated_text)
    } else {
        let error_msg = result.error.unwrap_or("ç”Ÿæˆæ–‡æœ¬å¤±æ•—".to_string());
        log::error!("ç”Ÿæˆæ–‡æœ¬å¤±æ•—: {}", error_msg);
        Err(error_msg)
    }
}

/// ä½¿ç”¨ä¸Šä¸‹æ–‡ç”Ÿæˆæ–‡æœ¬ï¼ˆå‚³çµ±ç‰ˆæœ¬ï¼‰
#[command]
pub async fn generate_with_context(
    project_id: String,
    chapter_id: String,
    position: usize,
    model: String,
    params: GenerateParams,
    language: Option<String>,
) -> Result<String, String> {
    log::info!("=== é–‹å§‹ä½¿ç”¨ä¸Šä¸‹æ–‡ç”Ÿæˆæ–‡æœ¬ ===");
    log::info!("å°ˆæ¡ˆ: {}, ç« ç¯€: {}, ä½ç½®: {}, æ¨¡å‹: {}, èªè¨€: {:?}", project_id, chapter_id, position, model, language);
    
    // ğŸ”¥ ä¿®å¾©ï¼šä½¿ç”¨æ–°çš„å¤šæä¾›è€…ç³»çµ±
    // é¦–å…ˆéœ€è¦æ‰¾åˆ°ä½¿ç”¨æ­¤æ¨¡å‹çš„æä¾›è€…
    use crate::database::connection::create_connection;
    use rusqlite::params;
    
    // ğŸ”¥ æ™ºèƒ½æä¾›è€…åŒ¹é…é‚è¼¯ - è®“ä¸€å€‹æä¾›è€…æ”¯æŒå¤šå€‹æ¨¡å‹
    let provider_id = {
        let conn = create_connection().map_err(|e| format!("è³‡æ–™åº«é€£æ¥å¤±æ•—: {}", e))?;
        
        // é¦–å…ˆå˜—è©¦ç²¾ç¢ºæ¨¡å‹åŒ¹é…ï¼ˆå‘å¾Œå…¼å®¹ï¼‰
        let mut stmt = conn.prepare(
            "SELECT id FROM ai_providers WHERE model = ?1 AND is_enabled = 1 LIMIT 1"
        ).map_err(|e| format!("æº–å‚™æŸ¥è©¢å¤±æ•—: {}", e))?;
        
        match stmt.query_row(params![model], |row| row.get::<_, String>(0)) {
            Ok(provider_id) => {
                log::info!("âœ… ç²¾ç¢ºåŒ¹é…æ‰¾åˆ°æä¾›è€…: {} for model: {}", provider_id, model);
                provider_id
            }
            Err(_) => {
                // ç²¾ç¢ºåŒ¹é…å¤±æ•—ï¼Œä½¿ç”¨æ™ºèƒ½åŒ¹é…
                log::info!("ğŸ“ ç²¾ç¢ºåŒ¹é…å¤±æ•—ï¼Œå˜—è©¦æ™ºèƒ½åŒ¹é…æ¨¡å‹: {}", model);
                
                let provider_type = if model.starts_with("gpt-") || model.contains("openai") {
                    "openai"
                } else if model.contains("gemini") || model.starts_with("gemini-") {
                    "gemini" 
                } else if model.contains("claude") || model.starts_with("claude-") {
                    "claude"
                } else if model.contains("/") { // OpenRouter format: provider/model
                    "openrouter"
                } else {
                    "ollama" // Default fallback
                };
                
                log::info!("ğŸ¯ æ™ºèƒ½åŒ¹é…: æ¨¡å‹ '{}' æ˜ å°„åˆ°æä¾›è€…é¡å‹ '{}'", model, provider_type);
                
                let mut stmt2 = conn.prepare(
                    "SELECT id FROM ai_providers WHERE provider_type = ?1 AND is_enabled = 1 LIMIT 1"
                ).map_err(|e| format!("æº–å‚™æ™ºèƒ½åŒ¹é…æŸ¥è©¢å¤±æ•—: {}", e))?;
                
                stmt2.query_row(params![provider_type], |row| row.get::<_, String>(0))
                    .map_err(|e| format!("æ™ºèƒ½åŒ¹é…å¤±æ•—: æ‰¾ä¸åˆ°é¡å‹ç‚º '{}' çš„å•Ÿç”¨æä¾›è€…ä¾†æ”¯æŒæ¨¡å‹ '{}'. è«‹å…ˆé…ç½®å°æ‡‰çš„AIæä¾›è€…ã€‚éŒ¯èª¤: {}", provider_type, model, e))?
            }
        }
    }; // åœ¨é€™è£¡é—œé–‰è³‡æ–™åº«é€£æ¥çš„ä½œç”¨åŸŸ
    
    log::info!("æ‰¾åˆ°æä¾›è€… ID: {}", provider_id);
    
    // ä½¿ç”¨æ–°çš„å¤šæä¾›è€…ç³»çµ±èª¿ç”¨ generate_ai_text
    let request = crate::commands::ai_providers::AIGenerationRequestData {
        provider_id,
        model,
        prompt: String::new(), // ç©ºçš„æç¤ºï¼Œè®“ç³»çµ±è‡ªå‹•æ§‹å»ºä¸Šä¸‹æ–‡çºŒå¯«
        system_prompt: None,
        project_id,
        chapter_id,
        position: Some(position),
        temperature: params.temperature.map(|x| x as f64),
        max_tokens: params.max_tokens.map(|x| x as i32),
        top_p: params.top_p.map(|x| x as f64),
        presence_penalty: params.presence_penalty.map(|x| x as f64),
        frequency_penalty: params.frequency_penalty.map(|x| x as f64),
        stop: None,
    };
    
    match crate::commands::ai_providers::generate_ai_text(request).await {
        Ok(result) => {
            if result.success {
                let generated_text = result.generated_text.unwrap_or_default();
                log::info!("ç”Ÿæˆæ–‡æœ¬æˆåŠŸï¼Œé•·åº¦: {} å­—ç¬¦", generated_text.len());
                Ok(generated_text)
            } else {
                let error_msg = result.error.unwrap_or("ç”Ÿæˆæ–‡æœ¬å¤±æ•—".to_string());
                log::error!("ç”Ÿæˆæ–‡æœ¬å¤±æ•—: {}", error_msg);
                Err(error_msg)
            }
        }
        Err(e) => {
            log::error!("èª¿ç”¨AIæä¾›è€…å¤±æ•—: {}", e);
            Err(e)
        }
    }
}

/// æ›´æ–° Ollama é…ç½®
#[command]
pub async fn update_ollama_config(config: UpdateConfigRequest) -> Result<ConfigUpdateResult, String> {
    let ollama_service = get_ollama_service();
    let mut service = ollama_service.lock().await;
    
    let update_config = UpdateConfig {
        base_url: config.base_url,
        timeout: config.timeout,
        retry_attempts: config.retry_attempts,
        retry_delay: config.retry_delay,
    };
    
    service.update_config(update_config);
    
    Ok(ConfigUpdateResult { success: true })
}

#[derive(Debug, Serialize, Deserialize)]
pub struct UpdateConfigRequest {
    pub base_url: Option<String>,
    pub timeout: Option<u64>,
    pub retry_attempts: Option<u32>,
    pub retry_delay: Option<u64>,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct ConfigUpdateResult {
    pub success: bool,
}