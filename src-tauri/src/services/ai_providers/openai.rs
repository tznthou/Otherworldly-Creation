use anyhow::{Result, anyhow};
use async_trait::async_trait;
use reqwest::Client;
use serde::{Deserialize, Serialize};
use std::time::Duration;
use std::collections::HashMap;

use super::r#trait::{
    AIProvider, ProviderConfig, AIGenerationRequest, AIGenerationResponse, 
    AIGenerationParams, AIUsageInfo, ModelInfo, detect_model_characteristics, ResponseFormat
};
use super::security::SecurityUtils;

#[derive(Debug, Serialize, Deserialize)]
struct OpenAIRequest {
    model: String,
    messages: Vec<OpenAIMessage>,
    temperature: f64,
    #[serde(skip_serializing_if = "Option::is_none")]
    max_tokens: Option<i32>,
    #[serde(skip_serializing_if = "Option::is_none")]
    max_completion_tokens: Option<i32>,
    #[serde(skip_serializing_if = "Option::is_none")]
    top_p: Option<f64>,
    #[serde(skip_serializing_if = "Option::is_none")]
    presence_penalty: Option<f64>,
    #[serde(skip_serializing_if = "Option::is_none")]
    frequency_penalty: Option<f64>,
    #[serde(skip_serializing_if = "Option::is_none")]
    stop: Option<Vec<String>>,
}

#[derive(Debug, Serialize, Deserialize)]
struct OpenAIMessage {
    role: String,
    content: Option<String>,
    #[serde(skip_serializing_if = "Option::is_none")]
    reasoning: Option<String>,
    #[serde(skip_serializing_if = "Option::is_none")]
    reasoning_summary: Option<String>,
}

#[derive(Debug, Serialize, Deserialize)]
struct OpenAIResponse {
    id: String,
    object: String,
    created: u64,
    model: String,
    choices: Vec<OpenAIChoice>,
    usage: OpenAIUsage,
}

#[derive(Debug, Serialize, Deserialize)]
struct OpenAIChoice {
    index: i32,
    message: OpenAIMessage,
    finish_reason: Option<String>,
}

#[derive(Debug, Serialize, Deserialize)]
struct OpenAIUsage {
    prompt_tokens: i32,
    completion_tokens: i32,
    total_tokens: i32,
}

#[derive(Debug, Serialize, Deserialize)]
struct OpenAIModelsResponse {
    object: String,
    data: Vec<OpenAIModelData>,
}

#[derive(Debug, Serialize, Deserialize)]
struct OpenAIModelData {
    id: String,
    object: String,
    created: u64,
    owned_by: String,
}

/// OpenAI API æä¾›è€…
#[allow(dead_code)]
pub struct OpenAIProvider {
    name: String,
    api_key: String,
    endpoint: String,
    model: String,
    client: Client,
    timeout: Duration,
    settings: HashMap<String, serde_json::Value>,
}

/// æª¢æ¸¬æ¨¡å‹æ˜¯å¦éœ€è¦ä½¿ç”¨æ–°çš„åƒæ•¸æ ¼å¼
fn is_new_api_model(model: &str) -> bool {
    // GPT-5 ç³»åˆ—å’ŒæŸäº›æ–°æ¨¡å‹ä½¿ç”¨ max_completion_tokens
    model.starts_with("gpt-5") || 
    model.starts_with("gpt-4o") ||
    model.contains("gpt-5-") ||
    model.contains("o1-") // OpenAI o1 ç³»åˆ—ä¹Ÿä½¿ç”¨æ–°æ ¼å¼
}

/// é©—è­‰ OpenAI æ¨¡å‹ ID æ˜¯å¦æœ‰æ•ˆ
fn is_valid_openai_model(model: &str) -> bool {
    const VALID_MODELS: &[&str] = &[
        "gpt-4o",
        "gpt-4o-mini", 
        "gpt-3.5-turbo",
        "gpt-4-turbo",
        "gpt-4",
        "gpt-4-0613",
        "gpt-4-32k",
        "gpt-4-32k-0613",
        "gpt-3.5-turbo-0613",
        "gpt-3.5-turbo-16k",
        "gpt-3.5-turbo-16k-0613",
        "gpt-3.5-turbo-1106",
        "gpt-3.5-turbo-0125",
        // æ–°æ¨¡å‹
        "gpt-4-turbo-preview",
        "gpt-4-0125-preview",
        "gpt-4-1106-preview",
        "gpt-4-vision-preview",
        "gpt-4-1106-vision-preview",
        // o1 ç³»åˆ—
        "o1-preview",
        "o1-mini",
    ];
    
    VALID_MODELS.contains(&model)
}

impl OpenAIProvider {
    pub fn new(config: &ProviderConfig) -> Result<Self> {
        let api_key = config.api_key
            .as_ref()
            .ok_or_else(|| anyhow!("OpenAI éœ€è¦ API é‡‘é‘°"))?
            .clone();
            
        log::info!("[OpenAIProvider] åˆå§‹åŒ–æä¾›è€…ï¼ŒAPIé‡‘é‘°: {}", SecurityUtils::mask_api_key(&api_key));

        let client = Client::builder()
            .timeout(Duration::from_secs(300))
            .build()
            .map_err(|e| anyhow!("å»ºç«‹ HTTP å®¢æˆ¶ç«¯å¤±æ•—: {}", e))?;

        let endpoint = config.endpoint
            .clone()
            .unwrap_or_else(|| "https://api.openai.com/v1".to_string());

        Ok(Self {
            name: config.name.clone(),
            api_key,
            endpoint,
            model: config.model.clone(),
            client,
            timeout: Duration::from_secs(120),
            settings: config.settings.clone(),
        })
    }

    /// å¾ reasoning ç›¸é—œæ¬„ä½æå–æ–‡æœ¬ï¼ˆå¢å¼·ç‰ˆï¼‰
    fn extract_text_from_reasoning_fields(choice: &OpenAIChoice) -> String {
        // æª¢æŸ¥ reasoning æ¬„ä½
        if let Some(reasoning) = &choice.message.reasoning {
            if !reasoning.trim().is_empty() {
                log::info!("[OpenAIProvider] âœ… ä½¿ç”¨ reasoning æ ¼å¼ï¼Œç”Ÿæˆ {} å­—ç¬¦", reasoning.len());
                return reasoning.clone();
            }
        }
        
        // æª¢æŸ¥ reasoning_summary æ¬„ä½
        if let Some(summary) = &choice.message.reasoning_summary {
            if !summary.trim().is_empty() {
                log::info!("[OpenAIProvider] âœ… ä½¿ç”¨ reasoning_summary æ ¼å¼ï¼Œç”Ÿæˆ {} å­—ç¬¦", summary.len());
                return summary.clone();
            }
        }
        
        // ğŸ”¥ æ–°å¢ï¼šå‹•æ…‹æª¢æŸ¥å…¶ä»–å¯èƒ½çš„æ¬„ä½åç¨±
        let message_value = serde_json::to_value(&choice.message).unwrap_or_default();
        let possible_fields = [
            "reasoning_content", "internal_thoughts", "chain_of_thought", 
            "thoughts", "reasoning_trace", "step_by_step", "analysis"
        ];
        
        for field_name in possible_fields {
            if let Some(value) = message_value.get(field_name) {
                if let Some(text) = value.as_str() {
                    if !text.trim().is_empty() {
                        log::info!("[OpenAIProvider] âœ… ä½¿ç”¨å‹•æ…‹æ¬„ä½ '{}' æ ¼å¼ï¼Œç”Ÿæˆ {} å­—ç¬¦", field_name, text.len());
                        return text.to_string();
                    }
                }
            }
        }
        
        // ğŸ”¥ æœ€å¾Œå˜—è©¦ï¼šæª¢æŸ¥æ˜¯å¦æœ‰ä»»ä½•å­—ç¬¦ä¸²æ¬„ä½åŒ…å«å…§å®¹
        if let Some(obj) = message_value.as_object() {
            for (key, value) in obj {
                if let Some(text) = value.as_str() {
                    if !text.trim().is_empty() && text.len() > 10 { // éæ¿¾æ‰å¤ªçŸ­çš„æ¬„ä½
                        log::warn!("[OpenAIProvider] ğŸ†˜ é™ç´šï¼šä½¿ç”¨æœªçŸ¥æ¬„ä½ '{}' æ ¼å¼ï¼Œç”Ÿæˆ {} å­—ç¬¦", key, text.len());
                        return text.to_string();
                    }
                }
            }
        }
        
        log::warn!("[OpenAIProvider] âš ï¸ æ‰€æœ‰æ¬„ä½æª¢æŸ¥å®Œç•¢ï¼Œæœªæ‰¾åˆ°æœ‰æ•ˆå…§å®¹");
        log::warn!("[OpenAIProvider] ğŸ” èª¿è©¦ï¼šmessageå®Œæ•´çµæ§‹ = {}", serde_json::to_string_pretty(&choice.message).unwrap_or_default());
        String::new()
    }

    /// ç™¼é€ GET è«‹æ±‚åˆ° OpenAI API
    async fn make_get_request<T>(&self, endpoint: &str) -> Result<T>
    where
        T: for<'de> Deserialize<'de>,
    {
        let url = format!("{}{}", self.endpoint, endpoint);
        let auth_header = format!("Bearer {}", self.api_key);
        log::debug!("[OpenAIProvider] ç™¼é€GETè«‹æ±‚åˆ°: {}, èªè­‰: {}", url, SecurityUtils::mask_api_key(&self.api_key));
        
        let response = self.client
            .get(&url)
            .header("Authorization", auth_header)
            .header("Content-Type", "application/json")
            .timeout(self.timeout)
            .send()
            .await?;

        if response.status().is_success() {
            let data = response.json::<T>().await?;
            Ok(data)
        } else {
            let status = response.status();
            let error_text = response.text().await.unwrap_or_default();
            let sanitized_error = SecurityUtils::sanitize_error_message(&error_text, &self.api_key);
            Err(anyhow!("OpenAI API éŒ¯èª¤ {}: {}", status, sanitized_error))
        }
    }

    /// ç™¼é€ POST è«‹æ±‚åˆ° OpenAI API
    async fn make_post_request<T>(&self, endpoint: &str, body: &impl Serialize) -> Result<T>
    where
        T: for<'de> Deserialize<'de>,
    {
        let url = format!("{}{}", self.endpoint, endpoint);
        let auth_header = format!("Bearer {}", self.api_key);
        log::debug!("[OpenAIProvider] ç™¼é€POSTè«‹æ±‚åˆ°: {}, èªè­‰: {}", url, SecurityUtils::mask_api_key(&self.api_key));
        
        let response = self.client
            .post(&url)
            .header("Authorization", auth_header)
            .header("Content-Type", "application/json")
            .json(body)
            .timeout(self.timeout)
            .send()
            .await?;

        if response.status().is_success() {
            let data = response.json::<T>().await?;
            Ok(data)
        } else {
            let status = response.status();
            let error_text = response.text().await.unwrap_or_default();
            let sanitized_error = SecurityUtils::sanitize_error_message(&error_text, &self.api_key);
            Err(anyhow!("OpenAI API éŒ¯èª¤ {}: {}", status, sanitized_error))
        }
    }

    /// ç²å–é å®šç¾©çš„ç†±é–€æ¨¡å‹åˆ—è¡¨ï¼ˆç”¨æ–¼é›¢ç·šæƒ…æ³ï¼‰
    fn get_popular_models() -> Vec<ModelInfo> {
        vec![
            ModelInfo {
                id: "gpt-4o".to_string(),
                name: "GPT-4o".to_string(),
                description: Some("æœ€æ–°çš„å¤šæ¨¡æ…‹æ¨¡å‹ï¼Œæ€§èƒ½å“è¶Š".to_string()),
                max_tokens: Some(128000),
                supports_streaming: true,
                cost_per_token: Some(0.000015), // è¼¸å…¥tokenæˆæœ¬
            },
            ModelInfo {
                id: "gpt-4o-mini".to_string(),
                name: "GPT-4o Mini".to_string(),
                description: Some("æ›´ç¶“æ¿Ÿå¯¦æƒ çš„GPT-4oç‰ˆæœ¬".to_string()),
                max_tokens: Some(128000),
                supports_streaming: true,
                cost_per_token: Some(0.00000015),
            },
            ModelInfo {
                id: "gpt-4-turbo".to_string(),
                name: "GPT-4 Turbo".to_string(),
                description: Some("é«˜æ€§èƒ½GPT-4æ¨¡å‹ï¼Œæ”¯æ´æ›´å¤§ä¸Šä¸‹æ–‡".to_string()),
                max_tokens: Some(128000),
                supports_streaming: true,
                cost_per_token: Some(0.00003),
            },
            ModelInfo {
                id: "gpt-3.5-turbo".to_string(),
                name: "GPT-3.5 Turbo".to_string(),
                description: Some("ç¶“æ¿Ÿå¯¦æƒ ä¸”å¿«é€Ÿçš„æ¨¡å‹".to_string()),
                max_tokens: Some(16384),
                supports_streaming: true,
                cost_per_token: Some(0.0000005),
            },
        ]
    }
}

#[async_trait]
impl AIProvider for OpenAIProvider {
    fn name(&self) -> &str {
        &self.name
    }
    
    fn provider_type(&self) -> &str {
        "openai"
    }

    async fn check_availability(&self) -> Result<bool> {
        log::info!("[OpenAIProvider] æª¢æŸ¥ OpenAI API å¯ç”¨æ€§ï¼Œä½¿ç”¨é‡‘é‘°: {}...", SecurityUtils::mask_api_key(&self.api_key));
        
        // å˜—è©¦ç²å–æ¨¡å‹åˆ—è¡¨ä¾†æ¸¬è©¦ API é‡‘é‘°
        match self.make_get_request::<OpenAIModelsResponse>("/models").await {
            Ok(_) => {
                log::info!("[OpenAIProvider] OpenAI API å¯ç”¨");
                Ok(true)
            }
            Err(e) => {
                log::warn!("[OpenAIProvider] OpenAI API ä¸å¯ç”¨: {}", e);
                Err(e)
            }
        }
    }

    async fn get_models(&self) -> Result<Vec<ModelInfo>> {
        log::info!("[OpenAIProvider] ç²å– OpenAI æ¨¡å‹åˆ—è¡¨...");
        
        // å…ˆå˜—è©¦å¾ API ç²å–
        match self.make_get_request::<OpenAIModelsResponse>("/models").await {
            Ok(response) => {
                let models: Vec<ModelInfo> = response.data.into_iter()
                    .filter(|model| {
                        // åªé¡¯ç¤º GPT æ¨¡å‹ï¼Œéæ¿¾å…¶ä»–é¡å‹
                        model.id.starts_with("gpt-")
                    })
                    .map(|model| {
                        // æ ¹æ“šæ¨¡å‹åç¨±æ¨æ¸¬æ€§èƒ½åƒæ•¸
                        let (max_tokens, cost_per_token) = if model.id.contains("gpt-4") {
                            if model.id.contains("turbo") || model.id.contains("4o") {
                                (Some(128000), Some(0.00003))
                            } else {
                                (Some(8192), Some(0.00006))
                            }
                        } else {
                            (Some(16384), Some(0.0000015))
                        };
                        
                        ModelInfo {
                            id: model.id.clone(),
                            name: model.id,
                            description: Some(format!("OpenAI æ¨¡å‹ï¼Œç”± {} æä¾›", model.owned_by)),
                            max_tokens,
                            supports_streaming: true,
                            cost_per_token,
                        }
                    })
                    .collect();
                
                log::info!("[OpenAIProvider] æˆåŠŸç²å– {} å€‹æ¨¡å‹", models.len());
                Ok(models)
            }
            Err(_) => {
                log::warn!("[OpenAIProvider] API ç²å–æ¨¡å‹å¤±æ•—ï¼Œä½¿ç”¨é å®šç¾©åˆ—è¡¨");
                Ok(Self::get_popular_models())
            }
        }
    }

    async fn generate_text(&self, request: AIGenerationRequest) -> Result<AIGenerationResponse> {
        // ğŸ”’ å®‰å…¨é©—è­‰ï¼šæª¢æŸ¥è¼¸å…¥åƒæ•¸
        SecurityUtils::validate_generation_params(&request.params)?;
        SecurityUtils::validate_prompt_content(&request.prompt, request.system_prompt.as_deref())?;
        
        // ğŸ”¥ æ–°å¢ï¼šé©—è­‰æ¨¡å‹ ID æ˜¯å¦æœ‰æ•ˆ
        if !is_valid_openai_model(&request.model) {
            return Err(anyhow!("ç„¡æ•ˆçš„ OpenAI æ¨¡å‹ ID: {}ã€‚æ”¯æ´çš„æ¨¡å‹: gpt-4o, gpt-4o-mini, gpt-3.5-turbo, gpt-4-turbo", request.model));
        }
        
        log::info!("[OpenAIProvider] âœ… é–‹å§‹ç”Ÿæˆæ–‡æœ¬ï¼Œæ¨¡å‹: {}, APIé‡‘é‘°: {}", request.model, SecurityUtils::mask_api_key(&self.api_key));

        // æ§‹å»ºè¨Šæ¯åˆ—è¡¨
        let mut messages = Vec::new();
        
        // æ·»åŠ ç³»çµ±æç¤ºï¼ˆå¦‚æœæœ‰ï¼‰
        if let Some(system_prompt) = &request.system_prompt {
            messages.push(OpenAIMessage {
                role: "system".to_string(),
                content: Some(system_prompt.clone()),
                reasoning: None,
                reasoning_summary: None,
            });
        }
        
        // æ·»åŠ ç”¨æˆ¶æç¤º
        messages.push(OpenAIMessage {
            role: "user".to_string(),
            content: Some(request.prompt),
            reasoning: None,
            reasoning_summary: None,
        });

        // ğŸ”¥ é—œéµä¿®å¾©ï¼šæ ¹æ“šæ¨¡å‹é¡å‹é¸æ“‡æ­£ç¢ºçš„åƒæ•¸
        let is_new_model = is_new_api_model(&request.model);
        
        let (max_tokens, max_completion_tokens) = if is_new_model {
            log::info!("[OpenAIProvider] ğŸ†• ä½¿ç”¨æ–°APIæ ¼å¼ (max_completion_tokens) å°æ¨¡å‹: {}", request.model);
            (None, Some(request.params.max_tokens))
        } else {
            log::info!("[OpenAIProvider] ğŸ“ ä½¿ç”¨èˆŠAPIæ ¼å¼ (max_tokens) å°æ¨¡å‹: {}", request.model);
            (Some(request.params.max_tokens), None)
        };

        // ğŸ”¥ æ–°ä¿®å¾©ï¼šGPT-5 ç³»åˆ—æ¨¡å‹åªæ¥å—ç‰¹å®šçš„åƒæ•¸å€¼
        let (temperature, top_p, presence_penalty, frequency_penalty, stop) = if is_new_model {
            log::info!("[OpenAIProvider] ğŸ¯ GPT-5 ç³»åˆ—æ¨¡å‹ï¼šä½¿ç”¨å›ºå®šåƒæ•¸ (temperature=1.0)");
            (1.0, None, None, None, None) // GPT-5 ç³»åˆ—åªæ¥å—é è¨­å€¼
        } else {
            log::info!("[OpenAIProvider] ğŸ›ï¸ å‚³çµ±æ¨¡å‹ï¼šä½¿ç”¨ç”¨æˆ¶è‡ªå®šç¾©åƒæ•¸");
            (
                request.params.temperature,
                request.params.top_p,
                request.params.presence_penalty,
                request.params.frequency_penalty,
                request.params.stop.clone(),
            )
        };

        let openai_request = OpenAIRequest {
            model: request.model.clone(),
            messages,
            temperature,
            max_tokens,
            max_completion_tokens,
            top_p,
            presence_penalty,
            frequency_penalty,
            stop,
        };

        let response = self.make_post_request::<OpenAIResponse>("/chat/completions", &openai_request).await?;
        
        // ğŸ”¥ è¨ºæ–·æ—¥å¿—ï¼šè¨˜éŒ„å®Œæ•´çš„APIå›æ‡‰
        log::debug!("[OpenAIProvider] ğŸ” å®Œæ•´APIå›æ‡‰: {}", serde_json::to_string_pretty(&response).unwrap_or_default());
        if let Some(choice) = response.choices.first() {
            log::debug!("[OpenAIProvider] ğŸ” messageçµæ§‹: {}", serde_json::to_string_pretty(&choice.message).unwrap_or_default());
        }
        
        // ğŸ”¥ ä½¿ç”¨éšæ®µä¸€æª¢æ¸¬é‚è¼¯è™•ç†éŸ¿æ‡‰æ ¼å¼å·®ç•°
        let model_chars = detect_model_characteristics(&request.model);
        let actual_text = match model_chars.response_format {
            ResponseFormat::Standard => {
                // OpenAI æ¨™æº–æ ¼å¼ï¼šå˜—è©¦å¤šå€‹æ¬„ä½
                if let Some(choice) = response.choices.first() {
                    // å„ªå…ˆæª¢æŸ¥ content æ¬„ä½
                    if let Some(content) = &choice.message.content {
                        if !content.trim().is_empty() {
                            log::info!("[OpenAIProvider] âœ… ä½¿ç”¨æ¨™æº– content æ ¼å¼ï¼Œç”Ÿæˆ {} å­—ç¬¦", content.len());
                            content.clone()
                        } else {
                            // content ç‚ºç©ºï¼Œæª¢æŸ¥å…¶ä»–æ¬„ä½
                            Self::extract_text_from_reasoning_fields(choice)
                        }
                    } else {
                        // content ä¸å­˜åœ¨ï¼Œæª¢æŸ¥å…¶ä»–æ¬„ä½
                        Self::extract_text_from_reasoning_fields(choice)
                    }
                } else {
                    log::warn!("[OpenAIProvider] âš ï¸ choices é™£åˆ—ç‚ºç©º");
                    String::new()
                }
            },
            _ => {
                // é™ç´šè™•ç†ï¼šå˜—è©¦å¾ choices ç²å–æ‰€æœ‰å¯èƒ½çš„æ¬„ä½
                if let Some(choice) = response.choices.first() {
                    log::info!("[OpenAIProvider] ğŸ“ é™ç´šè™•ç†ï¼šæª¢æŸ¥æ‰€æœ‰å¯èƒ½çš„æ¬„ä½");
                    
                    // æª¢æŸ¥ content
                    if let Some(content) = &choice.message.content {
                        if !content.trim().is_empty() {
                            content.clone()
                        } else {
                            Self::extract_text_from_reasoning_fields(choice)
                        }
                    } else {
                        Self::extract_text_from_reasoning_fields(choice)
                    }
                } else {
                    log::warn!("[OpenAIProvider] âš ï¸ ç„¡æ³•ç²å–ä»»ä½•éŸ¿æ‡‰å…§å®¹");
                    String::new()
                }
            }
        };
        
        if !actual_text.trim().is_empty() {
            log::info!("[OpenAIProvider] æ–‡æœ¬ç”ŸæˆæˆåŠŸï¼Œé•·åº¦: {} å­—ç¬¦", actual_text.len());
            
            let usage = AIUsageInfo {
                prompt_tokens: Some(response.usage.prompt_tokens),
                completion_tokens: Some(response.usage.completion_tokens),
                total_tokens: Some(response.usage.total_tokens),
            };

            Ok(AIGenerationResponse {
                text: actual_text,
                model: response.model,
                usage: Some(usage),
                finish_reason: response.choices.first()
                    .and_then(|c| c.finish_reason.clone()),
            })
        } else {
            Err(anyhow!("OpenAI API å›æ‡‰ä¸­æ²’æœ‰æœ‰æ•ˆå…§å®¹"))
        }
    }

    async fn validate_api_key(&self, api_key: &str) -> Result<bool> {
        log::info!("[OpenAIProvider] é©—è­‰ API é‡‘é‘°: {}...", SecurityUtils::mask_api_key(api_key));
        
        // å»ºç«‹è‡¨æ™‚å®¢æˆ¶ç«¯ä¾†æ¸¬è©¦ API é‡‘é‘°
        let temp_client = Client::new();
        let url = format!("{}/models", self.endpoint);
        let auth_header = format!("Bearer {}", api_key);
        
        let response = temp_client
            .get(&url)
            .header("Authorization", auth_header)
            .header("Content-Type", "application/json")
            .timeout(Duration::from_secs(30))
            .send()
            .await?;

        if response.status().is_success() {
            log::info!("[OpenAIProvider] API é‡‘é‘°é©—è­‰æˆåŠŸ");
            Ok(true)
        } else {
            log::warn!("[OpenAIProvider] API é‡‘é‘°é©—è­‰å¤±æ•—: {}", response.status());
            Ok(false)
        }
    }

    async fn estimate_cost(&self, request: &AIGenerationRequest) -> Result<Option<f64>> {
        // ç²—ç•¥ä¼°ç®—ï¼šè¼¸å…¥ + è¼¸å‡º token æˆæœ¬
        // é€™æ˜¯ä¸€å€‹ç°¡åŒ–çš„ä¼°ç®—ï¼Œå¯¦éš›æˆæœ¬å¯èƒ½æœ‰æ‰€ä¸åŒ
        let input_cost_per_token = if request.model.contains("gpt-4o-mini") {
            0.00000015
        } else if request.model.contains("gpt-4") {
            0.00003
        } else {
            0.0000015
        };
        
        let output_cost_per_token = input_cost_per_token * 2.0; // è¼¸å‡ºé€šå¸¸æ¯”è¼¸å…¥è²´
        
        // ä¼°ç®—è¼¸å…¥ token æ•¸ï¼ˆå¤§ç´„æ¯4å€‹å­—ç¬¦1å€‹tokenï¼‰
        let estimated_input_tokens = (request.prompt.len() / 4) as f64;
        let estimated_output_tokens = request.params.max_tokens as f64;
        
        let estimated_cost = (estimated_input_tokens * input_cost_per_token) + 
                           (estimated_output_tokens * output_cost_per_token);
        
        Ok(Some(estimated_cost))
    }

    fn default_params(&self) -> AIGenerationParams {
        AIGenerationParams {
            temperature: 0.7,
            max_tokens: 2000,
            top_p: Some(1.0),
            presence_penalty: Some(0.0),
            frequency_penalty: Some(0.0),
            stop: None,
        }
    }

    fn requires_api_key(&self) -> bool {
        true
    }

    fn supports_custom_endpoint(&self) -> bool {
        true // æ”¯æ´è‡ªè¨‚ç«¯é»ï¼Œé©ç”¨æ–¼ Azure OpenAI ç­‰
    }
}