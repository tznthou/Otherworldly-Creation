use anyhow::{Result, anyhow};
use async_trait::async_trait;
use reqwest::Client;
use serde::{Deserialize, Serialize};
use std::time::Duration;
use std::collections::HashMap;

use super::r#trait::{
    AIProvider, ProviderConfig, AIGenerationRequest, AIGenerationResponse, 
    AIGenerationParams, AIUsageInfo, ModelInfo, detect_model_characteristics, ResponseFormat
};
use super::security::SecurityUtils;

/// æª¢æ¸¬æ˜¯å¦ç‚º GPT-5 ç³»åˆ—æ¨¡å‹ï¼ˆéœ€è¦ç‰¹æ®Šåƒæ•¸è™•ç†ï¼‰
fn is_gpt5_model(model: &str) -> bool {
    let model_lower = model.to_lowercase();
    model_lower.contains("gpt-5") || 
    model_lower.starts_with("openai/gpt-5") ||
    model_lower.contains("gpt5")
}

/// æª¢æ¸¬æ˜¯å¦éœ€è¦ä½¿ç”¨ max_completion_tokens è€Œä¸æ˜¯ max_tokens
fn uses_completion_tokens_api(model: &str) -> bool {
    is_gpt5_model(model)
}

#[derive(Debug, Serialize, Deserialize)]
struct OpenRouterRequest {
    model: String,
    messages: Vec<OpenRouterMessage>,
    temperature: f64,
    #[serde(skip_serializing_if = "Option::is_none")]
    max_tokens: Option<i32>,
    #[serde(skip_serializing_if = "Option::is_none")]
    max_completion_tokens: Option<i32>,
    #[serde(skip_serializing_if = "Option::is_none")]
    top_p: Option<f64>,
    #[serde(skip_serializing_if = "Option::is_none")]
    presence_penalty: Option<f64>,
    #[serde(skip_serializing_if = "Option::is_none")]
    frequency_penalty: Option<f64>,
    #[serde(skip_serializing_if = "Option::is_none")]
    stop: Option<Vec<String>>,
}

#[derive(Debug, Serialize, Deserialize)]
struct OpenRouterMessage {
    role: String,
    content: String,
}

#[derive(Debug, Serialize, Deserialize)]
struct OpenRouterResponse {
    id: String,
    object: String,
    created: u64,
    model: String,
    choices: Vec<OpenRouterChoice>,
    usage: OpenRouterUsage,
}

#[derive(Debug, Serialize, Deserialize)]
struct OpenRouterChoice {
    index: i32,
    message: OpenRouterMessage,
    finish_reason: Option<String>,
}

#[derive(Debug, Serialize, Deserialize)]
struct OpenRouterUsage {
    prompt_tokens: i32,
    completion_tokens: i32,
    total_tokens: i32,
}

#[derive(Debug, Serialize, Deserialize)]
struct OpenRouterModelsResponse {
    data: Vec<OpenRouterModelData>,
}

#[derive(Debug, Serialize, Deserialize)]
struct OpenRouterModelData {
    id: String,
    name: String,
    description: Option<String>,
    context_length: Option<i32>,
    pricing: Option<OpenRouterPricing>,
}

#[derive(Debug, Serialize, Deserialize)]
struct OpenRouterPricing {
    prompt: String,
    completion: String,
}

/// OpenRouter API æä¾›è€…
#[allow(dead_code)]
pub struct OpenRouterProvider {
    name: String,
    api_key: String,
    endpoint: String,
    model: String,
    client: Client,
    timeout: Duration,
    settings: HashMap<String, serde_json::Value>,
}

impl OpenRouterProvider {
    pub fn new(config: &ProviderConfig) -> Result<Self> {
        let api_key = config.api_key
            .as_ref()
            .ok_or_else(|| anyhow!("OpenRouter éœ€è¦ API é‡‘é‘°"))?
            .clone();

        let client = Client::builder()
            .timeout(Duration::from_secs(300))
            .build()
            .map_err(|e| anyhow!("å»ºç«‹ HTTP å®¢æˆ¶ç«¯å¤±æ•—: {}", e))?;

        let endpoint = config.endpoint
            .clone()
            .unwrap_or_else(|| "https://openrouter.ai/api/v1".to_string());

        Ok(Self {
            name: config.name.clone(),
            api_key,
            endpoint,
            model: config.model.clone(),
            client,
            timeout: Duration::from_secs(180),
            settings: config.settings.clone(),
        })
    }

    /// ç™¼é€ GET è«‹æ±‚åˆ° OpenRouter API
    async fn make_get_request<T>(&self, endpoint: &str) -> Result<T>
    where
        T: for<'de> Deserialize<'de>,
    {
        let url = format!("{}{}", self.endpoint, endpoint);
        log::debug!("[OpenRouterProvider] ç™¼é€GETè«‹æ±‚åˆ°: {}, APIé‡‘é‘°: {}", url, SecurityUtils::mask_api_key(&self.api_key));
        
        let response = self.client
            .get(&url)
            .header("Authorization", format!("Bearer {}", self.api_key))
            .header("Content-Type", "application/json")
            .timeout(self.timeout)
            .send()
            .await?;

        if response.status().is_success() {
            let data = response.json::<T>().await?;
            Ok(data)
        } else {
            let status = response.status();
            let error_text = response.text().await.unwrap_or_default();
            Err(anyhow!("OpenRouter API éŒ¯èª¤ {}: {}", status, error_text))
        }
    }

    /// ç™¼é€ POST è«‹æ±‚åˆ° OpenRouter API
    async fn make_post_request<T>(&self, endpoint: &str, body: &impl Serialize) -> Result<T>
    where
        T: for<'de> Deserialize<'de>,
    {
        let url = format!("{}{}", self.endpoint, endpoint);
        log::debug!("[OpenRouterProvider] ç™¼é€POSTè«‹æ±‚åˆ°: {}, APIé‡‘é‘°: {}", url, SecurityUtils::mask_api_key(&self.api_key));
        
        let response = self.client
            .post(&url)
            .header("Authorization", format!("Bearer {}", self.api_key))
            .header("Content-Type", "application/json")
            .header("HTTP-Referer", "https://genesis-chronicle.app") // OpenRouter è¦æ±‚
            .header("X-Title", "Genesis Chronicle") // OpenRouter è¦æ±‚
            .json(body)
            .timeout(self.timeout)
            .send()
            .await?;

        if response.status().is_success() {
            let data = response.json::<T>().await?;
            Ok(data)
        } else {
            let status = response.status();
            let error_text = response.text().await.unwrap_or_default();
            Err(anyhow!("OpenRouter API éŒ¯èª¤ {}: {}", status, error_text))
        }
    }

    /// ç²å–ç†±é–€æ¨¡å‹çš„é å®šç¾©åˆ—è¡¨ï¼ˆç”¨æ–¼é›¢ç·šæƒ…æ³ï¼‰
    fn get_popular_models() -> Vec<ModelInfo> {
        vec![
            ModelInfo {
                id: "anthropic/claude-3.5-sonnet".to_string(),
                name: "Claude 3.5 Sonnet".to_string(),
                description: Some("Anthropic çš„æœ€æ–°å¼·å¤§æ¨¡å‹".to_string()),
                max_tokens: Some(200000),
                supports_streaming: true,
                cost_per_token: Some(0.000003),
            },
            ModelInfo {
                id: "openai/gpt-4o".to_string(),
                name: "GPT-4o".to_string(),
                description: Some("OpenAI çš„æœ€æ–°å¤šæ¨¡æ…‹æ¨¡å‹".to_string()),
                max_tokens: Some(128000),
                supports_streaming: true,
                cost_per_token: Some(0.000005),
            },
            ModelInfo {
                id: "google/gemini-pro-1.5".to_string(),
                name: "Gemini Pro 1.5".to_string(),
                description: Some("Google çš„å…ˆé€²èªè¨€æ¨¡å‹".to_string()),
                max_tokens: Some(1000000),
                supports_streaming: true,
                cost_per_token: Some(0.0000025),
            },
            ModelInfo {
                id: "meta-llama/llama-3.1-70b-instruct".to_string(),
                name: "Llama 3.1 70B Instruct".to_string(),
                description: Some("Meta çš„é–‹æºå¤§èªè¨€æ¨¡å‹".to_string()),
                max_tokens: Some(131072),
                supports_streaming: true,
                cost_per_token: Some(0.00000088),
            },
            ModelInfo {
                id: "mistralai/mixtral-8x7b-instruct".to_string(),
                name: "Mixtral 8x7B Instruct".to_string(),
                description: Some("Mistral AI çš„æ··åˆå°ˆå®¶æ¨¡å‹".to_string()),
                max_tokens: Some(32768),
                supports_streaming: true,
                cost_per_token: Some(0.00000024),
            },
        ]
    }

    /// è§£æåƒ¹æ ¼å­—ç¬¦ä¸²ç‚ºæµ®é»æ•¸
    fn parse_pricing(pricing_str: &str) -> Option<f64> {
        pricing_str.trim_start_matches('$').parse::<f64>().ok()
    }

    /// ğŸ”¥ é—œéµä¿®å¾©ï¼šæ ¼å¼åŒ–æ¨¡å‹IDç‚ºOpenRouterå…¼å®¹æ ¼å¼
    fn format_model_id(model_id: &str) -> String {
        // å¦‚æœå·²ç¶“æ˜¯æ­£ç¢ºæ ¼å¼ï¼ˆåŒ…å«æ–œæ ï¼‰ï¼Œç›´æ¥è¿”å›
        if model_id.contains('/') {
            return model_id.to_string();
        }
        
        // æ ¹æ“šæ¨¡å‹åç¨±æ¨æ–·æä¾›å•†å‰ç¶´
        if model_id.starts_with("gpt-") || model_id.starts_with("o1-") {
            format!("openai/{}", model_id)
        } else if model_id.starts_with("claude-") {
            format!("anthropic/{}", model_id)
        } else if model_id.starts_with("gemini-") {
            format!("google/{}", model_id)
        } else if model_id.starts_with("llama") {
            format!("meta-llama/{}", model_id)
        } else if model_id.starts_with("mixtral") {
            format!("mistralai/{}", model_id)
        } else {
            // æœªçŸ¥æ¨¡å‹ï¼Œå˜—è©¦æ¨æ–·æˆ–ä¿æŒåŸæ¨£
            log::warn!("[OpenRouterProvider] âš ï¸ æœªçŸ¥æ¨¡å‹æ ¼å¼ï¼Œä¿æŒåŸæ¨£: {}", model_id);
            model_id.to_string()
        }
    }
}

#[async_trait]
impl AIProvider for OpenRouterProvider {
    fn name(&self) -> &str {
        &self.name
    }
    
    fn provider_type(&self) -> &str {
        "openrouter"
    }

    async fn check_availability(&self) -> Result<bool> {
        log::info!("[OpenRouterProvider] æª¢æŸ¥ OpenRouter API å¯ç”¨æ€§...");
        
        // å˜—è©¦ç²å–æ¨¡å‹åˆ—è¡¨ä¾†æ¸¬è©¦ API é‡‘é‘°
        match self.make_get_request::<OpenRouterModelsResponse>("/models").await {
            Ok(_) => {
                log::info!("[OpenRouterProvider] OpenRouter API å¯ç”¨");
                Ok(true)
            }
            Err(e) => {
                log::warn!("[OpenRouterProvider] OpenRouter API ä¸å¯ç”¨: {}", e);
                Err(e)
            }
        }
    }

    async fn get_models(&self) -> Result<Vec<ModelInfo>> {
        log::info!("[OpenRouterProvider] ç²å– OpenRouter æ¨¡å‹åˆ—è¡¨...");
        
        // å…ˆå˜—è©¦å¾ API ç²å–
        match self.make_get_request::<OpenRouterModelsResponse>("/models").await {
            Ok(response) => {
                let models: Vec<ModelInfo> = response.data.into_iter()
                    .map(|model| {
                        let max_tokens = model.context_length;
                        let cost_per_token = model.pricing
                            .as_ref()
                            .and_then(|p| Self::parse_pricing(&p.prompt));
                        
                        ModelInfo {
                            id: model.id.clone(),
                            name: model.name,
                            description: model.description,
                            max_tokens,
                            supports_streaming: true, // OpenRouter å¤§éƒ¨åˆ†æ¨¡å‹æ”¯æŒä¸²æµ
                            cost_per_token,
                        }
                    })
                    .collect();
                
                log::info!("[OpenRouterProvider] æˆåŠŸç²å– {} å€‹æ¨¡å‹", models.len());
                Ok(models)
            }
            Err(_) => {
                log::warn!("[OpenRouterProvider] API ç²å–æ¨¡å‹å¤±æ•—ï¼Œä½¿ç”¨é å®šç¾©åˆ—è¡¨");
                Ok(Self::get_popular_models())
            }
        }
    }

    async fn generate_text(&self, request: AIGenerationRequest) -> Result<AIGenerationResponse> {
        // ğŸ”’ å®‰å…¨é©—è­‰ï¼šæª¢æŸ¥è¼¸å…¥åƒæ•¸
        SecurityUtils::validate_generation_params(&request.params)?;
        SecurityUtils::validate_prompt_content(&request.prompt, request.system_prompt.as_deref())?;
        
        log::info!("[OpenRouterProvider] ğŸš€ é–‹å§‹ç”Ÿæˆæ–‡æœ¬ï¼Œæ¨¡å‹: {}, APIé‡‘é‘°: {}", request.model, SecurityUtils::mask_api_key(&self.api_key));
        log::info!("[OpenRouterProvider] ğŸ” è«‹æ±‚åƒæ•¸: temperature={}, max_tokens={}, prompté•·åº¦={}", 
            request.params.temperature, request.params.max_tokens, request.prompt.len());
        
        // ğŸ”¥ é—œéµä¿®å¾©ï¼šæ™ºèƒ½èª¿æ•´max_tokensé˜²æ­¢ç©ºå›æ‡‰
        // æ›´ç²¾ç¢ºçš„tokenä¼°ç®—ï¼šä¸­æ–‡å­—ç¬¦ç´„2å€‹å­—ç¬¦=1tokenï¼Œè‹±æ–‡ç´„4å€‹å­—ç¬¦=1token
        let char_count = request.prompt.chars().count();
        let estimated_prompt_tokens = if char_count > 0 {
            // æ··åˆä¼°ç®—ï¼šå‡è¨­50%ä¸­æ–‡50%è‹±æ–‡
            (char_count as f64 * 0.75) as usize  // ä¿å®ˆä¼°ç®—
        } else {
            0
        };
        
        // ğŸ¯ åŸºæ–¼æ¨¡å‹ç‰¹æ€§çš„æ™ºèƒ½èª¿æ•´ç­–ç•¥
        let mut adjusted_max_tokens = request.params.max_tokens;
        let model_lower = request.model.to_lowercase();
        
        // ğŸ”¥ æ–°å¢ï¼šç¢ºä¿ max_tokens è‡³å°‘ç‚º 200ï¼Œé¿å…éä½å°è‡´ç©ºå›æ‡‰
        if adjusted_max_tokens < 200 {
            adjusted_max_tokens = 200;
            log::warn!("[OpenRouterProvider] âš ï¸ max_tokens éä½ ({}), æå‡è‡³ 200 é¿å…ç©ºå›æ‡‰", request.params.max_tokens);
        }
        
        // ğŸ”¥ ä¿®å¾©ï¼šé‡æ–°åˆ†é¡æ¨¡å‹é¡å‹ï¼Œç‰¹åˆ¥è™•ç† GPT-5 nano
        let is_gpt5_nano = model_lower.contains("gpt-5") && model_lower.contains("nano");
        let is_traditional_nano = (model_lower.contains("nano") || model_lower.contains("mini")) && !is_gpt5_nano;
        let is_large_model = model_lower.contains("4o") || model_lower.contains("claude") || model_lower.contains("gemini");
        
        // æ ¹æ“šæ¨¡å‹é¡å‹å’Œè¼¸å…¥é•·åº¦èª¿æ•´ç­–ç•¥
        if estimated_prompt_tokens > 300 {  // é™ä½è§¸ç™¼é–¾å€¼
            let (multiplier, max_limit) = if is_gpt5_nano {
                // ğŸ”¥ GPT-5 nanoï¼šç‰¹æ®Šç­–ç•¥ï¼Œæ¯”å‚³çµ± nano æ¨¡å‹æ›´å¼·å¤§
                let mult = if estimated_prompt_tokens > 2000 { 3.5 }
                          else if estimated_prompt_tokens > 1000 { 2.5 }
                          else { 2.0 };
                (mult, 3000.0)  // ğŸ“ˆ æé«˜ä¸Šé™åˆ° 3000
            } else if is_traditional_nano {
                // å‚³çµ± Nano æ¨¡å‹ï¼šæ›´ä¿å®ˆçš„ç­–ç•¥
                let mult = if estimated_prompt_tokens > 1500 { 2.5 }
                          else if estimated_prompt_tokens > 800 { 2.0 }
                          else { 1.5 };
                (mult, 1500.0)  // æ›´ä½çš„ä¸Šé™
            } else if is_large_model {
                // å¤§å‹æ¨¡å‹ï¼šæ›´æ¿€é€²çš„ç­–ç•¥
                let mult = if estimated_prompt_tokens > 2000 { 4.0 }
                          else if estimated_prompt_tokens > 1000 { 3.0 }
                          else { 2.0 };
                (mult, 4000.0)
            } else {
                // æ¨™æº–æ¨¡å‹ï¼šå¹³è¡¡ç­–ç•¥
                let mult = if estimated_prompt_tokens > 1500 { 3.0 }
                          else if estimated_prompt_tokens > 800 { 2.0 }
                          else { 1.5 };
                (mult, 2500.0)
            };
            
            adjusted_max_tokens = (request.params.max_tokens as f64 * multiplier).min(max_limit) as i32;
            log::warn!("[OpenRouterProvider] âš ï¸ æ¨¡å‹: {}, æç¤ºè©è¼ƒé•· (~{} tokens)ï¼Œèª¿æ•´max_tokens: {} -> {} (ç­–ç•¥: {})", 
                request.model, estimated_prompt_tokens, request.params.max_tokens, adjusted_max_tokens,
                if is_gpt5_nano { "GPT-5 nanoå¢å¼·" } 
                else if is_traditional_nano { "å‚³çµ±nanoä¿å®ˆ" } 
                else if is_large_model { "å¤§å‹æ¿€é€²" } 
                else { "æ¨™æº–å¹³è¡¡" });
        }
        
        log::info!("[OpenRouterProvider] ğŸ“Š Tokenåˆ†é…: å­—ç¬¦æ•¸={}, é ä¼°è¼¸å…¥~{} tokens, èª¿æ•´å¾Œè¼¸å‡ºé™åˆ¶={} tokens", 
            char_count, estimated_prompt_tokens, adjusted_max_tokens);

        // æ§‹å»ºè¨Šæ¯åˆ—è¡¨
        let mut messages = Vec::new();
        
        // æ·»åŠ ç³»çµ±æç¤ºï¼ˆå¦‚æœæœ‰ï¼‰
        if let Some(system_prompt) = &request.system_prompt {
            messages.push(OpenRouterMessage {
                role: "system".to_string(),
                content: system_prompt.clone(),
            });
        }
        
        // æ·»åŠ ç”¨æˆ¶æç¤º
        messages.push(OpenRouterMessage {
            role: "user".to_string(),
            content: request.prompt,
        });

        // ğŸ”¥ é—œéµä¿®å¾©ï¼šç¢ºä¿æ¨¡å‹IDæ ¼å¼æ­£ç¢º
        let formatted_model = Self::format_model_id(&request.model);
        log::info!("[OpenRouterProvider] ğŸ”§ æ¨¡å‹IDè½‰æ›: {} -> {}", request.model, formatted_model);
        
        // ğŸ”¥ æ–°å¢ï¼šGPT-5 æ¨¡å‹ç‰¹æ®Šåƒæ•¸è™•ç†
        let is_gpt5 = is_gpt5_model(&formatted_model);
        let uses_completion_api = uses_completion_tokens_api(&formatted_model);
        
        let (final_temperature, final_top_p, final_presence_penalty, final_frequency_penalty, final_stop) = if is_gpt5 {
            log::info!("[OpenRouterProvider] ğŸ¯ GPT-5 æ¨¡å‹ï¼šä½¿ç”¨å›ºå®šåƒæ•¸ (temperature=1.0)");
            (1.0, None, None, None, None) // GPT-5 ç³»åˆ—åªæ¥å—é è¨­å€¼
        } else {
            (request.params.temperature, request.params.top_p, request.params.presence_penalty, request.params.frequency_penalty, request.params.stop)
        };

        let (max_tokens, max_completion_tokens) = if uses_completion_api {
            log::info!("[OpenRouterProvider] ğŸ†• ä½¿ç”¨æ–°APIæ ¼å¼ (max_completion_tokens) å°æ¨¡å‹: {}", formatted_model);
            (None, Some(adjusted_max_tokens))
        } else {
            log::info!("[OpenRouterProvider] ğŸ“ ä½¿ç”¨èˆŠAPIæ ¼å¼ (max_tokens) å°æ¨¡å‹: {}", formatted_model);
            (Some(adjusted_max_tokens), None)
        };
        
        let openrouter_request = OpenRouterRequest {
            model: formatted_model.clone(),
            messages,
            temperature: final_temperature,
            max_tokens,
            max_completion_tokens,
            top_p: final_top_p,
            presence_penalty: final_presence_penalty,
            frequency_penalty: final_frequency_penalty,
            stop: final_stop,
        };

        // ğŸ”¥ æ–°å¢ï¼šè¨˜éŒ„å®Œæ•´è«‹æ±‚ä»¥ä¾¿èª¿è©¦
        log::debug!("[OpenRouterProvider] ğŸ“¤ ç™¼é€è«‹æ±‚: {}", serde_json::to_string(&openrouter_request).unwrap_or_default());

        let response = self.make_post_request::<OpenRouterResponse>("/chat/completions", &openrouter_request).await?;
        
        // ğŸ” èª¿è©¦ï¼šè¨˜éŒ„å®Œæ•´éŸ¿æ‡‰
        log::info!("[OpenRouterProvider] ğŸ” API éŸ¿æ‡‰: æ¨¡å‹={}, choicesæ•¸é‡={}", 
            response.model, response.choices.len());
        if let Some(choice) = response.choices.first() {
            log::info!("[OpenRouterProvider] ğŸ” ç¬¬ä¸€å€‹choice: contenté•·åº¦={}, finish_reason={:?}", 
                choice.message.content.len(), choice.finish_reason);
            // ğŸ”¥ æ–°å¢ï¼šå¦‚æœå…§å®¹ç‚ºç©ºï¼Œè¨˜éŒ„è©³ç´°ä¿¡æ¯
            if choice.message.content.trim().is_empty() {
                log::error!("[OpenRouterProvider] âŒ æ¨¡å‹ {} è¿”å›ç©ºå…§å®¹ï¼choice è©³æƒ…: {:?}", formatted_model, choice);
            }
        }
        
        // ğŸ”¥ ä½¿ç”¨éšæ®µä¸€æª¢æ¸¬é‚è¼¯è™•ç†éŸ¿æ‡‰æ ¼å¼å·®ç•°
        let model_chars = detect_model_characteristics(&request.model);
        let actual_text = match model_chars.response_format {
            ResponseFormat::Standard => {
                // OpenRouter æ¨™æº–æ ¼å¼ï¼ˆé¡ä¼¼ OpenAIï¼‰ï¼šchoices[0].message.content
                if let Some(choice) = response.choices.first() {
                    if !choice.message.content.trim().is_empty() {
                        log::info!("[OpenRouterProvider] âœ… ä½¿ç”¨æ¨™æº– choices æ ¼å¼ï¼Œç”Ÿæˆ {} å­—ç¬¦", choice.message.content.len());
                        choice.message.content.clone()
                    } else {
                        log::warn!("[OpenRouterProvider] âš ï¸ choices ä¸­æ–‡æœ¬ç‚ºç©º");
                        String::new()
                    }
                } else {
                    log::warn!("[OpenRouterProvider] âš ï¸ choices é™£åˆ—ç‚ºç©º");
                    String::new()
                }
            },
            ResponseFormat::ThinkingField => {
                // ç‰¹æ®Šè™•ç†ï¼šOpenRouter ä¸Šçš„æ€ç¶­æ¨¡å‹å¯èƒ½æœ‰ä¸åŒéŸ¿æ‡‰æ ¼å¼
                log::info!("[OpenRouterProvider] ğŸ“ æª¢æ¸¬åˆ°æ€ç¶­æ¨¡å‹ï¼Œä½¿ç”¨æ¨™æº–é™ç´šè™•ç†");
                if let Some(choice) = response.choices.first() {
                    choice.message.content.clone()
                } else {
                    log::warn!("[OpenRouterProvider] âš ï¸ æ€ç¶­æ¨¡å‹éŸ¿æ‡‰ç‚ºç©º");
                    String::new()
                }
            },
            _ => {
                // é™ç´šè™•ç†ï¼šå˜—è©¦å¾ choices ç²å–
                if let Some(choice) = response.choices.first() {
                    log::info!("[OpenRouterProvider] ğŸ“ é™ç´šä½¿ç”¨ choices æ ¼å¼");
                    choice.message.content.clone()
                } else {
                    log::warn!("[OpenRouterProvider] âš ï¸ ç„¡æ³•ç²å–ä»»ä½•éŸ¿æ‡‰å…§å®¹");
                    String::new()
                }
            }
        };
        
        if !actual_text.trim().is_empty() {
            log::info!("[OpenRouterProvider] âœ… æ–‡æœ¬ç”ŸæˆæˆåŠŸï¼Œé•·åº¦: {} å­—ç¬¦", actual_text.len());
            
            let usage = AIUsageInfo {
                prompt_tokens: Some(response.usage.prompt_tokens),
                completion_tokens: Some(response.usage.completion_tokens),
                total_tokens: Some(response.usage.total_tokens),
            };

            Ok(AIGenerationResponse {
                text: actual_text,
                model: response.model,
                usage: Some(usage),
                finish_reason: response.choices.first()
                    .and_then(|c| c.finish_reason.clone()),
            })
        } else {
            // ğŸ”¥ æ–°å¢ï¼šæ›´è©³ç´°çš„éŒ¯èª¤ä¿¡æ¯ï¼ŒåŒ…å«å¯èƒ½çš„è§£æ±ºæ–¹æ¡ˆ
            let error_details = if let Some(choice) = response.choices.first() {
                format!("æ¨¡å‹ {} å›æ‡‰ç‚ºç©ºã€‚finish_reason: {:?}, max_tokens: {}, æ¨¡å‹å¯èƒ½éœ€è¦æ›´å¤š tokens æˆ–ä¸åŒçš„æç¤ºæ ¼å¼ã€‚å»ºè­°å˜—è©¦å¢åŠ  max_tokens æˆ–æª¢æŸ¥æ¨¡å‹æ˜¯å¦æ”¯æŒæ­¤é¡å‹çš„è«‹æ±‚ã€‚", 
                    Self::format_model_id(&request.model), choice.finish_reason, adjusted_max_tokens)
            } else {
                format!("æ¨¡å‹ {} æœªè¿”å›ä»»ä½• choicesã€‚è«‹æª¢æŸ¥æ¨¡å‹ ID æ˜¯å¦æ­£ç¢ºæˆ–è©²æ¨¡å‹æ˜¯å¦å¯ç”¨ã€‚", Self::format_model_id(&request.model))
            };
            
            log::error!("[OpenRouterProvider] âŒ {}", error_details);
            Err(anyhow!("OpenRouter API å›æ‡‰ä¸­æ²’æœ‰æœ‰æ•ˆå…§å®¹: {}", error_details))
        }
    }

    async fn validate_api_key(&self, api_key: &str) -> Result<bool> {
        log::info!("[OpenRouterProvider] é©—è­‰ API é‡‘é‘°...");
        
        // å»ºç«‹è‡¨æ™‚å®¢æˆ¶ç«¯ä¾†æ¸¬è©¦ API é‡‘é‘°
        let temp_client = Client::new();
        let url = format!("{}/models", self.endpoint);
        
        let response = temp_client
            .get(&url)
            .header("Authorization", format!("Bearer {}", api_key))
            .header("Content-Type", "application/json")
            .timeout(Duration::from_secs(30))
            .send()
            .await?;

        if response.status().is_success() {
            log::info!("[OpenRouterProvider] API é‡‘é‘°é©—è­‰æˆåŠŸ");
            Ok(true)
        } else {
            log::warn!("[OpenRouterProvider] API é‡‘é‘°é©—è­‰å¤±æ•—: {}", response.status());
            Ok(false)
        }
    }

    async fn estimate_cost(&self, request: &AIGenerationRequest) -> Result<Option<f64>> {
        // OpenRouter çš„æˆæœ¬å–æ±ºæ–¼å…·é«”æ¨¡å‹ï¼Œé€™è£¡æä¾›ç²—ç•¥ä¼°ç®—
        let base_cost_per_token = if request.model.contains("claude") {
            0.000008
        } else if request.model.contains("gpt-4") {
            0.00003
        } else if request.model.contains("gemini") {
            0.0000025
        } else if request.model.contains("llama") {
            0.0000004
        } else {
            0.000001 // é è¨­å€¼
        };
        
        // ä¼°ç®—tokenæ•¸
        let estimated_input_tokens = (request.prompt.len() / 4) as f64;
        let estimated_output_tokens = request.params.max_tokens as f64;
        
        let estimated_cost = (estimated_input_tokens + estimated_output_tokens) * base_cost_per_token;
        
        Ok(Some(estimated_cost))
    }

    fn default_params(&self) -> AIGenerationParams {
        AIGenerationParams {
            temperature: 0.7,
            max_tokens: 2000,
            top_p: Some(1.0),
            presence_penalty: Some(0.0),
            frequency_penalty: Some(0.0),
            stop: None,
        }
    }

    fn requires_api_key(&self) -> bool {
        true
    }

    fn supports_custom_endpoint(&self) -> bool {
        false // OpenRouter ä½¿ç”¨æ¨™æº–ç«¯é»
    }
}