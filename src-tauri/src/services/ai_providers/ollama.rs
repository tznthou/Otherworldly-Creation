use anyhow::{Result, anyhow};
use async_trait::async_trait;
use reqwest::Client;
use serde::{Deserialize, Serialize};
use std::time::Duration;
use std::collections::HashMap;

use super::r#trait::{
    AIProvider, ProviderConfig, AIGenerationRequest, AIGenerationResponse, 
    AIGenerationParams, AIUsageInfo, ModelInfo
};

#[derive(Debug, Serialize, Deserialize)]
struct OllamaModel {
    pub name: String,
    pub size: u64,
    pub digest: String,
    pub modified_at: String,
}

#[derive(Debug, Serialize, Deserialize)]
struct OllamaResponse {
    pub models: Vec<OllamaModel>,
}

#[derive(Debug, Serialize, Deserialize)]
struct OllamaVersionResponse {
    pub version: String,
}

#[derive(Debug, Serialize, Deserialize)]
struct OllamaGenerateRequest {
    pub model: String,
    pub prompt: String,
    pub stream: bool,
    pub options: Option<OllamaOptions>,
}

#[derive(Debug, Serialize, Deserialize)]
struct OllamaOptions {
    pub temperature: Option<f32>,
    pub top_p: Option<f32>,
    #[serde(rename = "num_predict")]
    pub max_tokens: Option<u32>,  // Ollama ä½¿ç”¨ num_predict è€Œä¸æ˜¯ max_tokens
    pub presence_penalty: Option<f32>,
    pub frequency_penalty: Option<f32>,
}

#[derive(Debug, Serialize, Deserialize)]
struct OllamaGenerateResponse {
    pub response: String,
    pub done: bool,
    pub thinking: Option<String>,  // ğŸ”¥ æ–°å¢ï¼šæŸäº›æ¨¡å‹æœƒä½¿ç”¨é€™å€‹å­—æ®µ
    pub context: Option<Vec<i32>>,
    pub total_duration: Option<u64>,
    pub load_duration: Option<u64>,
    pub prompt_eval_count: Option<u32>,
    pub prompt_eval_duration: Option<u64>,
    pub eval_count: Option<u32>,
    pub eval_duration: Option<u64>,
}

/// éæ¿¾æ‰AIæ€è€ƒæ¨™ç±¤å’Œä¸ç•¶å…§å®¹çš„å‡½æ•¸
fn filter_thinking_tags(text: &str) -> String {
    use regex::Regex;
    
    log::info!("[OllamaProvider] ğŸ” é–‹å§‹éæ¿¾ï¼ŒåŸå§‹æ–‡æœ¬é•·åº¦: {} å­—ç¬¦", text.len());
    log::debug!("[OllamaProvider] ğŸ” åŸå§‹æ–‡æœ¬ï¼ˆå‰500å­—ç¬¦ï¼‰: {}", text.chars().take(500).collect::<String>());
    
    let mut filtered_text = text.to_string();
    
    // ğŸ”¥ Step 1: ç§»é™¤æ€è€ƒæ¨™ç±¤
    let thinking_patterns = vec![
        r"(?s)<think[^>]*>.*?</think>",
        r"(?s)<thinking[^>]*>.*?</thinking>", 
        r"(?s)```thinking.*?```",
        r"(?s)\[THINKING\].*?\[/THINKING\]",
    ];
    
    for (i, pattern) in thinking_patterns.iter().enumerate() {
        if let Ok(re) = Regex::new(pattern) {
            let before_len = filtered_text.len();
            filtered_text = re.replace_all(&filtered_text, "").to_string();
            let after_len = filtered_text.len();
            if before_len != after_len {
                log::info!("[OllamaProvider] ğŸ¯ æ€è€ƒæ¨¡å¼ {} ç§»é™¤äº† {} å­—ç¬¦", i+1, before_len - after_len);
            }
        }
    }
    
    // ğŸ”¥ Step 2: åªç§»é™¤æ˜ç¢ºçš„è‹±æ–‡æŒ‡å°èªå¥è¡Œ
    let english_lines = [
        "We need to continue",
        "The existing text ends with:",
        "We need to insert",
        "The story is about",
        "Should be in Traditional Chinese",
        "Let's write:",
        "Provide continuation",
        "We have to insert",
        "The prompt gives",
        "Actually the content is",
        "The insertion point is",
        "JSON",
        "paragraph with some characters",
    ];
    
    for (i, line) in english_lines.iter().enumerate() {
        // åªç§»é™¤åŒ…å«é€™äº›çŸ­èªçš„å®Œæ•´è¡Œ
        let pattern = format!(r"(?m)^.*{}.*$
?", regex::escape(line));
        if let Ok(re) = Regex::new(&pattern) {
            let before_len = filtered_text.len();
            filtered_text = re.replace_all(&filtered_text, "").to_string();
            let after_len = filtered_text.len();
            if before_len != after_len {
                log::info!("[OllamaProvider] ğŸ¯ è‹±æ–‡è¡Œ {} '{}' ç§»é™¤äº† {} å­—ç¬¦", i+1, line, before_len - after_len);
            }
        }
    }
    
    // ğŸ”¥ Step 3: æ¸…ç†æ ¼å¼
    filtered_text = filtered_text.replace("\n", "
");
    filtered_text = filtered_text.replace("\\n", "
");
    
    // æ¸…ç†éå¤šç©ºè¡Œ
    if let Ok(re) = Regex::new(r"
{3,}") {
        filtered_text = re.replace_all(&filtered_text, "

").to_string();
    }
    
    // ç§»é™¤åŒ…åœå¼•è™Ÿ
    if (filtered_text.starts_with('"') && filtered_text.ends_with('"') && filtered_text.len() > 2) ||
       (filtered_text.starts_with('\'') && filtered_text.ends_with('\'') && filtered_text.len() > 2) {
        filtered_text = filtered_text[1..filtered_text.len()-1].to_string();
        log::info!("[OllamaProvider] ğŸ¯ ç§»é™¤äº†åŒ…åœå¼•è™Ÿ");
    }
    
    filtered_text = filtered_text.trim().to_string();
    
    // ğŸ”¥ Step 4: å“è³ªæª¢æŸ¥å’Œå¾Œå‚™ç­–ç•¥
    if filtered_text.is_empty() {
        log::warn!("[OllamaProvider] âš ï¸ éæ¿¾å¾Œæ–‡æœ¬ç‚ºç©ºï¼Œä½¿ç”¨å¾Œå‚™ç­–ç•¥");
        // å¾Œå‚™ï¼šåªç§»é™¤æ˜é¡¯çš„æ€è€ƒæ¨™ç±¤
        let mut backup_text = text.to_string();
        if let Ok(re) = Regex::new(r"(?s)<thinking>.*?</thinking>") {
            backup_text = re.replace_all(&backup_text, "").to_string();
        }
        if let Ok(re) = Regex::new(r"(?s)<think>.*?</think>") {
            backup_text = re.replace_all(&backup_text, "").to_string();
        }
        backup_text = backup_text.trim().to_string();
        log::warn!("[OllamaProvider] ğŸ”„ å¾Œå‚™ç­–ç•¥æ–‡æœ¬é•·åº¦: {} å­—ç¬¦", backup_text.len());
        return backup_text;
    }
    
    log::info!("[OllamaProvider] âœ… éæ¿¾å®Œæˆï¼Œè¿”å›æ–‡æœ¬é•·åº¦: {} å­—ç¬¦", filtered_text.len());
    if filtered_text.len() < 50 {
        log::debug!("[OllamaProvider] ğŸ” æœ€çµ‚æ–‡æœ¬å…§å®¹: '{}'", filtered_text);
    }
    
    filtered_text
}

/// Ollama AI æä¾›è€…
pub struct OllamaProvider {
    name: String,
    endpoint: String,
    model: String,
    client: Client,
    timeout: Duration,
    retry_attempts: u32,
    retry_delay: Duration,
    settings: HashMap<String, serde_json::Value>,
}

impl OllamaProvider {
    pub fn new(config: &ProviderConfig) -> Result<Self> {
        let client = Client::builder()
            .timeout(Duration::from_secs(400))  // å¢åŠ åˆ° 6.5 åˆ†é˜ï¼Œçµ¦ AI ç”Ÿæˆè¶³å¤ æ™‚é–“
            .build()
            .map_err(|e| anyhow!("å»ºç«‹ HTTP å®¢æˆ¶ç«¯å¤±æ•—: {}", e))?;

        // è‡ªå‹•å°‡ localhost è½‰æ›ç‚º 127.0.0.1 ä»¥é¿å… IPv6 è§£æå•é¡Œ
        let endpoint = config.endpoint
            .clone()
            .unwrap_or_else(|| "http://127.0.0.1:11434".to_string())
            .replace("localhost", "127.0.0.1");

        Ok(Self {
            name: config.name.clone(),
            endpoint,
            model: config.model.clone(),
            client,
            timeout: Duration::from_secs(300),  // API å‘¼å« timeout è¨­ç‚º 5 åˆ†é˜
            retry_attempts: 2,
            retry_delay: Duration::from_millis(500),
            settings: config.settings.clone(),
        })
    }

    /// ç™¼é€ GET è«‹æ±‚
    async fn make_get_request<T>(&self, endpoint: &str) -> Result<T>
    where
        T: for<'de> Deserialize<'de>,
    {
        let url = format!("{}{}", self.endpoint, endpoint);
        let response = self.client
            .get(&url)
            .timeout(self.timeout)
            .send()
            .await?;

        if response.status().is_success() {
            let data = response.json::<T>().await?;
            Ok(data)
        } else {
            Err(anyhow!("HTTP {}: {}", response.status(), response.status().canonical_reason().unwrap_or("Unknown")))
        }
    }

    /// ç™¼é€ POST è«‹æ±‚
    async fn make_post_request<T>(&self, endpoint: &str, body: &impl Serialize) -> Result<T>
    where
        T: for<'de> Deserialize<'de>,
    {
        let url = format!("{}{}", self.endpoint, endpoint);
        let response = self.client
            .post(&url)
            .json(body)
            .timeout(self.timeout)
            .send()
            .await?;

        if response.status().is_success() {
            let data = response.json::<T>().await?;
            Ok(data)
        } else {
            Err(anyhow!("HTTP {}: {}", response.status(), response.status().canonical_reason().unwrap_or("Unknown")))
        }
    }
}

#[async_trait]
impl AIProvider for OllamaProvider {
    fn name(&self) -> &str {
        &self.name
    }
    
    fn provider_type(&self) -> &str {
        "ollama"
    }

    async fn check_availability(&self) -> Result<bool> {
        log::info!("[OllamaProvider] é–‹å§‹æª¢æŸ¥æœå‹™å¯ç”¨æ€§...");
        
        match self.make_get_request::<OllamaVersionResponse>("/api/version").await {
            Ok(version_response) => {
                log::info!("[OllamaProvider] æœå‹™æª¢æŸ¥æˆåŠŸï¼Œç‰ˆæœ¬: {}", version_response.version);
                Ok(true)
            }
            Err(e) => {
                let error_msg = e.to_string();
                log::warn!("[OllamaProvider] æœå‹™æª¢æŸ¥å¤±æ•—: {}", error_msg);
                
                if error_msg.contains("Connection refused") {
                    Err(anyhow!("Ollama æœå‹™æœªå•Ÿå‹•ï¼Œè«‹åŸ·è¡Œ ollama serve"))
                } else if error_msg.contains("timeout") {
                    Err(anyhow!("Ollama æœå‹™éŸ¿æ‡‰è¶…æ™‚"))
                } else {
                    Err(e)
                }
            }
        }
    }

    async fn get_models(&self) -> Result<Vec<ModelInfo>> {
        log::info!("[OllamaProvider] ç²å–æ¨¡å‹åˆ—è¡¨...");
        
        let response = self.make_get_request::<OllamaResponse>("/api/tags").await?;
        
        let models: Vec<ModelInfo> = response.models.into_iter().map(|model| {
            ModelInfo {
                id: model.name.clone(),
                name: model.name,
                description: None,
                max_tokens: Some(4096), // Ollama é è¨­ä¸Šä¸‹æ–‡é•·åº¦
                supports_streaming: true,
                cost_per_token: None, // Ollama æœ¬åœ°é‹è¡Œå…è²»
            }
        }).collect();

        log::info!("[OllamaProvider] ç²å–åˆ° {} å€‹æ¨¡å‹", models.len());
        Ok(models)
    }

    async fn generate_text(&self, request: AIGenerationRequest) -> Result<AIGenerationResponse> {
        log::info!("[OllamaProvider] é–‹å§‹ç”Ÿæˆæ–‡æœ¬ï¼Œæ¨¡å‹: {}", request.model);
        log::info!("[OllamaProvider] åŸå§‹è«‹æ±‚å…§å®¹é•·åº¦: {} å­—ç¬¦", request.prompt.len());

        // å…ˆæª¢æŸ¥æœå‹™å¯ç”¨æ€§
        self.check_availability().await?;

        // è½‰æ›åƒæ•¸æ ¼å¼
        let options = OllamaOptions {
            temperature: Some(request.params.temperature as f32),
            top_p: request.params.top_p.map(|v| v as f32),
            max_tokens: Some(request.params.max_tokens as u32),
            presence_penalty: request.params.presence_penalty.map(|v| v as f32),
            frequency_penalty: request.params.frequency_penalty.map(|v| v as f32),
        };

        // æ§‹å»ºå®Œæ•´æç¤ºè©ï¼ˆåŒ…å«ç³»çµ±æç¤ºï¼‰
        let full_prompt = if let Some(system_prompt) = &request.system_prompt {
            format!("{}

{}", system_prompt, request.prompt)
        } else {
            request.prompt.clone()
        };
        
        log::info!("[OllamaProvider] æœ€çµ‚æç¤ºè©é•·åº¦: {} å­—ç¬¦", full_prompt.len());
        log::info!("[OllamaProvider] æœ€çµ‚æç¤ºè©å…§å®¹ï¼ˆå‰200å­—ç¬¦ï¼‰: {}", 
                   full_prompt.chars().take(200).collect::<String>());

        let request_body = OllamaGenerateRequest {
            model: request.model.clone(),
            prompt: full_prompt,
            stream: false,
            options: Some(options),
        };

        // é‡è©¦æ©Ÿåˆ¶
        let mut last_error = String::new();
        for attempt in 1..=self.retry_attempts {
            match self.make_post_request::<OllamaGenerateResponse>("/api/generate", &request_body).await {
                Ok(response) => {
                    log::info!("[OllamaProvider] æ–‡æœ¬ç”ŸæˆæˆåŠŸ");
                    log::info!("[OllamaProvider] ğŸ” åŸå§‹å›æ‡‰: response.response = '{}'", response.response);
                    log::info!("[OllamaProvider] ğŸ” å›æ‡‰é•·åº¦: {} å­—ç¬¦", response.response.len());
                    log::info!("[OllamaProvider] ğŸ” done: {}", response.done);
                    log::info!("[OllamaProvider] ğŸ” prompt_eval_count: {:?}", response.prompt_eval_count);
                    log::info!("[OllamaProvider] ğŸ” eval_count: {:?}", response.eval_count);
                    
                    let usage = AIUsageInfo {
                        prompt_tokens: response.prompt_eval_count.map(|v| v as i32),
                        completion_tokens: response.eval_count.map(|v| v as i32),
                        total_tokens: response.prompt_eval_count
                            .and_then(|p| response.eval_count.map(|c| (p + c) as i32)),
                    };

                    // ğŸ”¥ ä¿®å¾©ï¼šè¨˜éŒ„ thinking å­—æ®µä½†ä¸ä½¿ç”¨å®ƒä½œç‚ºè¼¸å‡º
                    if let Some(thinking_text) = &response.thinking {
                        log::debug!("[OllamaProvider] ğŸ§  æª¢æ¸¬åˆ° thinking å­—æ®µï¼Œé•·åº¦: {} å­—ç¬¦ï¼ˆåƒ…ç”¨æ–¼èª¿è©¦ï¼‰", thinking_text.len());
                        log::debug!("[OllamaProvider] ğŸ§  thinking å…§å®¹ï¼ˆå‰100å­—ç¬¦ï¼‰: {}", 
                                   thinking_text.chars().take(100).collect::<String>());
                    }

                    // ğŸ”¥ æ™ºèƒ½è™•ç†ï¼šå„ªå…ˆä½¿ç”¨ response å­—æ®µï¼Œå¦‚æœç‚ºç©ºå‰‡ä½¿ç”¨éæ¿¾å¾Œçš„ thinking å­—æ®µ
                    let actual_text = if !response.response.trim().is_empty() {
                        log::info!("[OllamaProvider] âœ… ä½¿ç”¨ response å­—æ®µä½œç‚ºæœ€çµ‚è¼¸å‡ºï¼Œé•·åº¦: {} å­—ç¬¦", response.response.len());
                        response.response.clone()
                    } else if let Some(thinking_text) = &response.thinking {
                        if !thinking_text.trim().is_empty() {
                            log::info!("[OllamaProvider] ğŸ“ response å­—æ®µç‚ºç©ºï¼Œä½¿ç”¨ thinking å­—æ®µä¸¦éæ¿¾æ€è€ƒæ¨™ç±¤");
                            log::debug!("[OllamaProvider] ğŸ” åŸå§‹ thinking å…§å®¹ï¼ˆå‰200å­—ç¬¦ï¼‰: {}", 
                                       thinking_text.chars().take(200).collect::<String>());
                            let filtered_text = filter_thinking_tags(thinking_text);
                            log::info!("[OllamaProvider] ğŸ¯ éæ¿¾å¾Œçš„æ–‡æœ¬é•·åº¦: {} å­—ç¬¦", filtered_text.len());
                            if !filtered_text.trim().is_empty() {
                                filtered_text
                            } else {
                                log::warn!("[OllamaProvider] âš ï¸ éæ¿¾å¾Œçš„æ–‡æœ¬ç‚ºç©º");
                                String::new()
                            }
                        } else {
                            log::warn!("[OllamaProvider] âš ï¸ thinking å­—æ®µä¹Ÿç‚ºç©º");
                            String::new()
                        }
                    } else {
                        log::warn!("[OllamaProvider] âš ï¸ response å’Œ thinking å­—æ®µéƒ½ç‚ºç©º");
                        String::new()
                    };

                    log::info!("[OllamaProvider] ğŸ¯ æœ€çµ‚ä½¿ç”¨çš„æ–‡æœ¬é•·åº¦: {} å­—ç¬¦", actual_text.len());

                    return Ok(AIGenerationResponse {
                        text: actual_text,
                        model: request.model,
                        usage: Some(usage),
                        finish_reason: if response.done { Some("stop".to_string()) } else { None },
                    });
                }
                Err(e) => {
                    last_error = e.to_string();
                    log::warn!("[OllamaProvider] ç”Ÿæˆæ–‡æœ¬å˜—è©¦ {}/{} å¤±æ•—: {}", attempt, self.retry_attempts, last_error);
                    
                    if attempt < self.retry_attempts {
                        tokio::time::sleep(self.retry_delay).await;
                    }
                }
            }
        }

        Err(anyhow!("ç”Ÿæˆæ–‡æœ¬å¤±æ•— ({} æ¬¡å˜—è©¦): {}", self.retry_attempts, last_error))
    }

    async fn validate_api_key(&self, _api_key: &str) -> Result<bool> {
        // Ollama ä¸éœ€è¦ API é‡‘é‘°ï¼Œç›´æ¥æª¢æŸ¥æœå‹™å¯ç”¨æ€§
        self.check_availability().await
    }

    async fn estimate_cost(&self, _request: &AIGenerationRequest) -> Result<Option<f64>> {
        // Ollama æœ¬åœ°é‹è¡Œå…è²»
        Ok(Some(0.0))
    }

    fn default_params(&self) -> AIGenerationParams {
        AIGenerationParams {
            temperature: 0.7,
            max_tokens: 2000,
            top_p: Some(0.9),
            presence_penalty: Some(0.0),
            frequency_penalty: Some(0.0),
            stop: None,
        }
    }

    fn requires_api_key(&self) -> bool {
        false // Ollama ä¸éœ€è¦ API é‡‘é‘°
    }

    fn supports_custom_endpoint(&self) -> bool {
        true // Ollama æ”¯æ´è‡ªè¨‚ç«¯é»
    }
}